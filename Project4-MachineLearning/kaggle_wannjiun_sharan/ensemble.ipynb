{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from seaborn.linearmodels import *\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred)) \n",
    "\n",
    "path = './data/'\n",
    "train_file = path + 'train.csv'\n",
    "test_file = path + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456, 97)\n",
      "(1459, 97)\n",
      "('Training set size:', (1456, 389))\n",
      "('Test set size:', (1459, 389))\n"
     ]
    }
   ],
   "source": [
    "# There are a few houses with more than 4000 sq ft living area that are\n",
    "# outliers, so we drop them from the training data. There is also one in\n",
    "# the test set but we obviously can't drop that one.\n",
    "train_df.drop(train_df[train_df[\"GrLivArea\"] > 4000].index, inplace=True)\n",
    "\n",
    "# The test example with ID 666 has GarageArea, GarageCars, and GarageType \n",
    "# but none of the other fields, so use the mode and median to fill them in.\n",
    "test_df.loc[666, \"GarageQual\"] = \"TA\"\n",
    "test_df.loc[666, \"GarageCond\"] = \"TA\"\n",
    "test_df.loc[666, \"GarageFinish\"] = \"Unf\"\n",
    "test_df.loc[666, \"GarageYrBlt\"] = \"1980\"\n",
    "\n",
    "# The test example 1116 only has GarageType but no other information. We'll \n",
    "# assume it does not have a garage.\n",
    "test_df.loc[1116, \"GarageType\"] = np.nan\n",
    "\n",
    "# For imputing missing values: fill in missing LotFrontage values by the median\n",
    "# LotFrontage of the neighborhood.\n",
    "lot_frontage_by_neighborhood = train_df[\"LotFrontage\"].groupby(train_df[\"Neighborhood\"])\n",
    "\n",
    "# Used to convert categorical features into ordinal numbers.\n",
    "# (There's probably an easier way to do this, but it works.)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "def factorize(df, factor_df, column, fill_na=None):\n",
    "    factor_df[column] = df[column]\n",
    "    if fill_na is not None:\n",
    "        factor_df[column].fillna(fill_na, inplace=True)\n",
    "    le.fit(factor_df[column].unique())\n",
    "    factor_df[column] = le.transform(factor_df[column])\n",
    "    return factor_df\n",
    "\n",
    "# Combine all the (numerical) features into one big DataFrame. We don't add \n",
    "# the one-hot encoded variables here yet, that happens later on.\n",
    "def munge(df):\n",
    "    all_df = pd.DataFrame(index = df.index)\n",
    "   \n",
    "    all_df[\"LotFrontage\"] = df[\"LotFrontage\"]   \n",
    "    for key, group in lot_frontage_by_neighborhood:\n",
    "        idx = (df[\"Neighborhood\"] == key) & (df[\"LotFrontage\"].isnull())\n",
    "        all_df.loc[idx, \"LotFrontage\"] = group.median()    \n",
    "\n",
    "    all_df[\"LotArea\"] = df[\"LotArea\"]\n",
    "\n",
    "    all_df[\"MasVnrArea\"] = df[\"MasVnrArea\"]\n",
    "    all_df[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "   \n",
    "    all_df[\"BsmtFinSF1\"] = df[\"BsmtFinSF1\"]\n",
    "    all_df[\"BsmtFinSF1\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"BsmtFinSF2\"] = df[\"BsmtFinSF2\"]\n",
    "    all_df[\"BsmtFinSF2\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"BsmtUnfSF\"] = df[\"BsmtUnfSF\"]\n",
    "    all_df[\"BsmtUnfSF\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"TotalBsmtSF\"] = df[\"TotalBsmtSF\"]\n",
    "    all_df[\"TotalBsmtSF\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"1stFlrSF\"] = df[\"1stFlrSF\"]\n",
    "    all_df[\"2ndFlrSF\"] = df[\"2ndFlrSF\"]\n",
    "    all_df[\"GrLivArea\"] = df[\"GrLivArea\"]\n",
    "    \n",
    "    all_df[\"GarageArea\"] = df[\"GarageArea\"]\n",
    "    all_df[\"GarageArea\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"WoodDeckSF\"] = df[\"WoodDeckSF\"]\n",
    "    all_df[\"OpenPorchSF\"] = df[\"OpenPorchSF\"]\n",
    "    all_df[\"EnclosedPorch\"] = df[\"EnclosedPorch\"]\n",
    "    all_df[\"3SsnPorch\"] = df[\"3SsnPorch\"]\n",
    "    all_df[\"ScreenPorch\"] = df[\"ScreenPorch\"]\n",
    "    \n",
    "    all_df[\"BsmtFullBath\"] = df[\"BsmtFullBath\"]\n",
    "    all_df[\"BsmtFullBath\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"BsmtHalfBath\"] = df[\"BsmtHalfBath\"]\n",
    "    all_df[\"BsmtHalfBath\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"FullBath\"] = df[\"FullBath\"] \n",
    "    all_df[\"HalfBath\"] = df[\"HalfBath\"] \n",
    "    all_df[\"BedroomAbvGr\"] = df[\"BedroomAbvGr\"] \n",
    "    all_df[\"KitchenAbvGr\"] = df[\"KitchenAbvGr\"] \n",
    "    all_df[\"TotRmsAbvGrd\"] = df[\"TotRmsAbvGrd\"] \n",
    "    all_df[\"Fireplaces\"] = df[\"Fireplaces\"] \n",
    "\n",
    "    all_df[\"GarageCars\"] = df[\"GarageCars\"]\n",
    "    all_df[\"GarageCars\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"CentralAir\"] = (df[\"CentralAir\"] == \"Y\") * 1.0\n",
    "   \n",
    "    all_df[\"OverallQual\"] = df[\"OverallQual\"]\n",
    "    all_df[\"OverallCond\"] = df[\"OverallCond\"]\n",
    "\n",
    "    # Quality measurements are stored as text but we can convert them to \n",
    "    # numbers where a higher number means higher quality.\n",
    "\n",
    "    qual_dict = {None: 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "    all_df[\"ExterQual\"] = df[\"ExterQual\"].map(qual_dict).astype(int)\n",
    "    all_df[\"ExterCond\"] = df[\"ExterCond\"].map(qual_dict).astype(int)\n",
    "    all_df[\"BsmtQual\"] = df[\"BsmtQual\"].map(qual_dict).astype(int)\n",
    "    all_df[\"BsmtCond\"] = df[\"BsmtCond\"].map(qual_dict).astype(int)\n",
    "    all_df[\"HeatingQC\"] = df[\"HeatingQC\"].map(qual_dict).astype(int)\n",
    "    all_df[\"KitchenQual\"] = df[\"KitchenQual\"].map(qual_dict).astype(int)\n",
    "    all_df[\"FireplaceQu\"] = df[\"FireplaceQu\"].map(qual_dict).astype(int)\n",
    "    all_df[\"GarageQual\"] = df[\"GarageQual\"].map(qual_dict).astype(int)\n",
    "    all_df[\"GarageCond\"] = df[\"GarageCond\"].map(qual_dict).astype(int)\n",
    "\n",
    "    all_df[\"BsmtExposure\"] = df[\"BsmtExposure\"].map(\n",
    "        {None: 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n",
    "\n",
    "    bsmt_fin_dict = {None: 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "    all_df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\n",
    "    all_df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\n",
    "\n",
    "    all_df[\"Functional\"] = df[\"Functional\"].map(\n",
    "        {None: 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n",
    "         \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n",
    "\n",
    "    all_df[\"GarageFinish\"] = df[\"GarageFinish\"].map(\n",
    "        {None: 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\n",
    "\n",
    "    all_df[\"Fence\"] = df[\"Fence\"].map(\n",
    "        {None: 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n",
    "\n",
    "    all_df[\"YearBuilt\"] = df[\"YearBuilt\"]\n",
    "    all_df[\"YearRemodAdd\"] = df[\"YearRemodAdd\"]\n",
    "\n",
    "    all_df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"]\n",
    "    all_df[\"GarageYrBlt\"].fillna(0.0, inplace=True)\n",
    "\n",
    "    all_df[\"MoSold\"] = df[\"MoSold\"]\n",
    "    all_df[\"YrSold\"] = df[\"YrSold\"]\n",
    "    \n",
    "    all_df[\"LowQualFinSF\"] = df[\"LowQualFinSF\"]\n",
    "    all_df[\"MiscVal\"] = df[\"MiscVal\"]\n",
    "\n",
    "    all_df[\"PoolQC\"] = df[\"PoolQC\"].map(qual_dict).astype(int)\n",
    "\n",
    "    all_df[\"PoolArea\"] = df[\"PoolArea\"]\n",
    "    all_df[\"PoolArea\"].fillna(0, inplace=True)\n",
    "    \n",
    "#     # Add categorical features as numbers too. It seems to help a bit.\n",
    "#     all_df = factorize(df, all_df, \"MSSubClass\")\n",
    "#     all_df = factorize(df, all_df, \"MSZoning\", \"RL\")\n",
    "#     all_df = factorize(df, all_df, \"LotConfig\")\n",
    "#     all_df = factorize(df, all_df, \"Neighborhood\")\n",
    "#     all_df = factorize(df, all_df, \"Condition1\")\n",
    "#     all_df = factorize(df, all_df, \"BldgType\")\n",
    "#     all_df = factorize(df, all_df, \"HouseStyle\")\n",
    "#     all_df = factorize(df, all_df, \"RoofStyle\")\n",
    "#     all_df = factorize(df, all_df, \"Exterior1st\", \"Other\")\n",
    "#     all_df = factorize(df, all_df, \"Exterior2nd\", \"Other\")\n",
    "#     all_df = factorize(df, all_df, \"MasVnrType\", \"None\")\n",
    "#     all_df = factorize(df, all_df, \"Foundation\")\n",
    "#     all_df = factorize(df, all_df, \"SaleType\", \"Oth\")\n",
    "#     all_df = factorize(df, all_df, \"SaleCondition\")\n",
    "\n",
    "    # IR2 and IR3 don't appear that often, so just make a distinction\n",
    "    # between regular and irregular.\n",
    "    all_df[\"IsRegularLotShape\"] = (df[\"LotShape\"] == \"Reg\") * 1\n",
    "\n",
    "    # Most properties are level; bin the other possibilities together\n",
    "    # as \"not level\".\n",
    "    all_df[\"IsLandLevel\"] = (df[\"LandContour\"] == \"Lvl\") * 1\n",
    "\n",
    "    # Most land slopes are gentle; treat the others as \"not gentle\".\n",
    "    all_df[\"IsLandSlopeGentle\"] = (df[\"LandSlope\"] == \"Gtl\") * 1\n",
    "\n",
    "    # Most properties use standard circuit breakers.\n",
    "    all_df[\"IsElectricalSBrkr\"] = (df[\"Electrical\"] == \"SBrkr\") * 1\n",
    "\n",
    "    # About 2/3rd have an attached garage.\n",
    "    all_df[\"IsGarageDetached\"] = (df[\"GarageType\"] == \"Detchd\") * 1\n",
    "\n",
    "    # Most have a paved drive. Treat dirt/gravel and partial pavement\n",
    "    # as \"not paved\".\n",
    "    all_df[\"IsPavedDrive\"] = (df[\"PavedDrive\"] == \"Y\") * 1\n",
    "\n",
    "    # The only interesting \"misc. feature\" is the presence of a shed.\n",
    "    all_df[\"HasShed\"] = (df[\"MiscFeature\"] == \"Shed\") * 1.  \n",
    "\n",
    "    # If YearRemodAdd != YearBuilt, then a remodeling took place at some point.\n",
    "    all_df[\"Remodeled\"] = (all_df[\"YearRemodAdd\"] != all_df[\"YearBuilt\"]) * 1\n",
    "    \n",
    "    # Did a remodeling happen in the year the house was sold?\n",
    "    all_df[\"RecentRemodel\"] = (all_df[\"YearRemodAdd\"] == all_df[\"YrSold\"]) * 1\n",
    "    \n",
    "    # Was this house sold in the year it was built?\n",
    "    all_df[\"VeryNewHouse\"] = (all_df[\"YearBuilt\"] == all_df[\"YrSold\"]) * 1\n",
    "\n",
    "    all_df[\"Has2ndFloor\"] = (all_df[\"2ndFlrSF\"] == 0) * 1\n",
    "    all_df[\"HasMasVnr\"] = (all_df[\"MasVnrArea\"] == 0) * 1\n",
    "    all_df[\"HasWoodDeck\"] = (all_df[\"WoodDeckSF\"] == 0) * 1\n",
    "    all_df[\"HasOpenPorch\"] = (all_df[\"OpenPorchSF\"] == 0) * 1\n",
    "    all_df[\"HasEnclosedPorch\"] = (all_df[\"EnclosedPorch\"] == 0) * 1\n",
    "    all_df[\"Has3SsnPorch\"] = (all_df[\"3SsnPorch\"] == 0) * 1\n",
    "    all_df[\"HasScreenPorch\"] = (all_df[\"ScreenPorch\"] == 0) * 1\n",
    "\n",
    "    # These features actually lower the score a little.\n",
    "    # all_df[\"HasBasement\"] = df[\"BsmtQual\"].isnull() * 1\n",
    "    # all_df[\"HasGarage\"] = df[\"GarageQual\"].isnull() * 1\n",
    "    # all_df[\"HasFireplace\"] = df[\"FireplaceQu\"].isnull() * 1\n",
    "    # all_df[\"HasFence\"] = df[\"Fence\"].isnull() * 1\n",
    "\n",
    "    # Months with the largest number of deals may be significant.\n",
    "    all_df[\"HighSeason\"] = df[\"MoSold\"].replace( \n",
    "        {1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})\n",
    "\n",
    "    all_df[\"NewerDwelling\"] = df[\"MSSubClass\"].replace(\n",
    "        {20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,\n",
    "         90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})   \n",
    "    \n",
    "    all_df.loc[df.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\n",
    "    all_df.loc[df.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\n",
    "    all_df.loc[df.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\n",
    "    all_df.loc[df.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\n",
    "    all_df.loc[df.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\n",
    "    all_df[\"Neighborhood_Good\"].fillna(0, inplace=True)\n",
    "\n",
    "    all_df[\"SaleCondition_PriceDown\"] = df.SaleCondition.replace(\n",
    "        {'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n",
    "\n",
    "    # House completed before sale or not\n",
    "    all_df[\"BoughtOffPlan\"] = df.SaleCondition.replace(\n",
    "        {\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n",
    "    \n",
    "    all_df[\"BadHeating\"] = df.HeatingQC.replace(\n",
    "        {'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})\n",
    "\n",
    "    area_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n",
    "                 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n",
    "    all_df[\"TotalArea\"] = all_df[area_cols].sum(axis=1)\n",
    "\n",
    "    all_df[\"TotalArea1st2nd\"] = all_df[\"1stFlrSF\"] + all_df[\"2ndFlrSF\"]\n",
    "\n",
    "    all_df[\"Age\"] = 2010 - all_df[\"YearBuilt\"]\n",
    "    all_df[\"TimeSinceSold\"] = 2010 - all_df[\"YrSold\"]\n",
    "\n",
    "    all_df[\"SeasonSold\"] = all_df[\"MoSold\"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, \n",
    "                                                  6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)\n",
    "    \n",
    "    all_df[\"YearsSinceRemodel\"] = all_df[\"YrSold\"] - all_df[\"YearRemodAdd\"]\n",
    "    \n",
    "    # Simplifications of existing features into bad/average/good.\n",
    "    all_df[\"SimplOverallQual\"] = all_df.OverallQual.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\n",
    "    all_df[\"SimplOverallCond\"] = all_df.OverallCond.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\n",
    "    all_df[\"SimplPoolQC\"] = all_df.PoolQC.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 2, 4 : 2})\n",
    "    all_df[\"SimplGarageCond\"] = all_df.GarageCond.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplGarageQual\"] = all_df.GarageQual.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplFireplaceQu\"] = all_df.FireplaceQu.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplFireplaceQu\"] = all_df.FireplaceQu.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplFunctional\"] = all_df.Functional.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})\n",
    "    all_df[\"SimplKitchenQual\"] = all_df.KitchenQual.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplHeatingQC\"] = all_df.HeatingQC.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplBsmtFinType1\"] = all_df.BsmtFinType1.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\n",
    "    all_df[\"SimplBsmtFinType2\"] = all_df.BsmtFinType2.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\n",
    "    all_df[\"SimplBsmtCond\"] = all_df.BsmtCond.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplBsmtQual\"] = all_df.BsmtQual.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplExterCond\"] = all_df.ExterCond.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "    all_df[\"SimplExterQual\"] = all_df.ExterQual.replace(\n",
    "        {1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "            \n",
    "    # Bin by neighborhood (a little arbitrarily). Values were computed by: \n",
    "    # train_df[\"SalePrice\"].groupby(train_df[\"Neighborhood\"]).median().sort_values()\n",
    "    neighborhood_map = {\n",
    "        \"MeadowV\" : 0,  #  88000\n",
    "        \"IDOTRR\" : 1,   # 103000\n",
    "        \"BrDale\" : 1,   # 106000\n",
    "        \"OldTown\" : 1,  # 119000\n",
    "        \"Edwards\" : 1,  # 119500\n",
    "        \"BrkSide\" : 1,  # 124300\n",
    "        \"Sawyer\" : 1,   # 135000\n",
    "        \"Blueste\" : 1,  # 137500\n",
    "        \"SWISU\" : 2,    # 139500\n",
    "        \"NAmes\" : 2,    # 140000\n",
    "        \"NPkVill\" : 2,  # 146000\n",
    "        \"Mitchel\" : 2,  # 153500\n",
    "        \"SawyerW\" : 2,  # 179900\n",
    "        \"Gilbert\" : 2,  # 181000\n",
    "        \"NWAmes\" : 2,   # 182900\n",
    "        \"Blmngtn\" : 2,  # 191000\n",
    "        \"CollgCr\" : 2,  # 197200\n",
    "        \"ClearCr\" : 3,  # 200250\n",
    "        \"Crawfor\" : 3,  # 200624\n",
    "        \"Veenker\" : 3,  # 218000\n",
    "        \"Somerst\" : 3,  # 225500\n",
    "        \"Timber\" : 3,   # 228475\n",
    "        \"StoneBr\" : 4,  # 278000\n",
    "        \"NoRidge\" : 4,  # 290000\n",
    "        \"NridgHt\" : 4,  # 315000\n",
    "    }\n",
    "\n",
    "    all_df[\"NeighborhoodBin\"] = df[\"Neighborhood\"].map(neighborhood_map)\n",
    "    return all_df\n",
    "\n",
    "train_df_munged = munge(train_df)\n",
    "test_df_munged = munge(test_df)\n",
    "\n",
    "print(train_df_munged.shape)\n",
    "print(test_df_munged.shape)\n",
    "\n",
    "# Copy NeighborhoodBin into a temporary DataFrame because we want to use the\n",
    "# unscaled version later on (to one-hot encode it). \n",
    "neighborhood_bin_train = pd.DataFrame(index = train_df.index)\n",
    "neighborhood_bin_train[\"NeighborhoodBin\"] = train_df_munged[\"NeighborhoodBin\"]\n",
    "neighborhood_bin_test = pd.DataFrame(index = test_df.index)\n",
    "neighborhood_bin_test[\"NeighborhoodBin\"] = test_df_munged[\"NeighborhoodBin\"]\n",
    "\n",
    "################################################################################\n",
    "\n",
    "numeric_features = train_df_munged.dtypes[train_df_munged.dtypes != \"object\"].index\n",
    "\n",
    "# Transform the skewed numeric features by taking log(feature + 1).\n",
    "# This will make the features more normal.\n",
    "from scipy.stats import skew\n",
    "\n",
    "skewed = train_df_munged[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\n",
    "skewed = skewed[skewed > 0.75]\n",
    "skewed = skewed.index\n",
    "\n",
    "train_df_munged[skewed] = np.log1p(train_df_munged[skewed])\n",
    "test_df_munged[skewed] = np.log1p(test_df_munged[skewed])\n",
    "\n",
    "# Additional processing: scale the data.   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df_munged[numeric_features])\n",
    "\n",
    "scaled = scaler.transform(train_df_munged[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    train_df_munged[col] = scaled[:, i]\n",
    "\n",
    "scaled = scaler.transform(test_df_munged[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    test_df_munged[col] = scaled[:, i]\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Convert categorical features using one-hot encoding.\n",
    "def onehot(onehot_df, df, column_name, fill_na, drop_name):\n",
    "    onehot_df[column_name] = df[column_name]\n",
    "    if fill_na is not None:\n",
    "        onehot_df[column_name].fillna(fill_na, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(onehot_df[column_name], prefix=\"_\" + column_name)\n",
    "    \n",
    "    # Dropping one of the columns actually made the results slightly worse.\n",
    "    # if drop_name is not None:\n",
    "    #     dummies.drop([\"_\" + column_name + \"_\" + drop_name], axis=1, inplace=True)\n",
    "\n",
    "    onehot_df = onehot_df.join(dummies)\n",
    "    onehot_df = onehot_df.drop([column_name], axis=1)\n",
    "    return onehot_df\n",
    "\n",
    "def munge_onehot(df):\n",
    "    onehot_df = pd.DataFrame(index = df.index)\n",
    "\n",
    "    onehot_df = onehot(onehot_df, df, \"MSSubClass\", None, \"40\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MSZoning\", \"RL\", \"RH\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LotConfig\", None, \"FR3\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Neighborhood\", None, \"OldTown\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Condition1\", None, \"RRNe\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BldgType\", None, \"2fmCon\")\n",
    "    onehot_df = onehot(onehot_df, df, \"HouseStyle\", None, \"1.5Unf\")\n",
    "    onehot_df = onehot(onehot_df, df, \"RoofStyle\", None, \"Shed\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Exterior1st\", \"VinylSd\", \"CBlock\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Exterior2nd\", \"VinylSd\", \"CBlock\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Foundation\", None, \"Wood\")\n",
    "    onehot_df = onehot(onehot_df, df, \"SaleType\", \"WD\", \"Oth\")\n",
    "    onehot_df = onehot(onehot_df, df, \"SaleCondition\", \"Normal\", \"AdjLand\")\n",
    "\n",
    "    # Fill in missing MasVnrType for rows that do have a MasVnrArea.\n",
    "    temp_df = df[[\"MasVnrType\", \"MasVnrArea\"]].copy()\n",
    "    idx = (df[\"MasVnrArea\"] != 0) & ((df[\"MasVnrType\"] == \"None\") | (df[\"MasVnrType\"].isnull()))\n",
    "    temp_df.loc[idx, \"MasVnrType\"] = \"BrkFace\"\n",
    "    onehot_df = onehot(onehot_df, temp_df, \"MasVnrType\", \"None\", \"BrkCmn\")\n",
    "\n",
    "    # Also add the booleans from calc_df as dummy variables.\n",
    "    onehot_df = onehot(onehot_df, df, \"LotShape\", None, \"IR3\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LandContour\", None, \"Low\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LandSlope\", None, \"Sev\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Electrical\", \"SBrkr\", \"FuseP\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageType\", \"None\", \"CarPort\")\n",
    "    onehot_df = onehot(onehot_df, df, \"PavedDrive\", None, \"P\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MiscFeature\", \"None\", \"Othr\")\n",
    "\n",
    "    # Features we can probably ignore (but want to include anyway to see\n",
    "    # if they make any positive difference).\n",
    "    # Definitely ignoring Utilities: all records are \"AllPub\", except for\n",
    "    # one \"NoSeWa\" in the train set and 2 NA in the test set.\n",
    "    onehot_df = onehot(onehot_df, df, \"Street\", None, \"Grvl\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Alley\", \"None\", \"Grvl\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Condition2\", None, \"PosA\")\n",
    "    onehot_df = onehot(onehot_df, df, \"RoofMatl\", None, \"WdShake\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Heating\", None, \"Wall\")\n",
    "\n",
    "    # I have these as numerical variables too.\n",
    "    onehot_df = onehot(onehot_df, df, \"ExterQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"ExterCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"HeatingQC\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"KitchenQual\", \"TA\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"FireplaceQu\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"PoolQC\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtExposure\", \"None\", \"Gd\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtFinType1\", \"None\", \"GLQ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtFinType2\", \"None\", \"GLQ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Functional\", \"Typ\", \"Typ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageFinish\", \"None\", \"Fin\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Fence\", \"None\", \"MnPrv\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MoSold\", None, None)\n",
    "    \n",
    "    # Divide up the years between 1871 and 2010 in slices of 20 years.\n",
    "    year_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20)) for i in range(0, 7))\n",
    "\n",
    "    yearbin_df = pd.DataFrame(index = df.index)\n",
    "    yearbin_df[\"GarageYrBltBin\"] = df.GarageYrBlt.map(year_map)\n",
    "    yearbin_df[\"GarageYrBltBin\"].fillna(\"NoGarage\", inplace=True)\n",
    "\n",
    "    yearbin_df[\"YearBuiltBin\"] = df.YearBuilt.map(year_map)\n",
    "    yearbin_df[\"YearRemodAddBin\"] = df.YearRemodAdd.map(year_map)\n",
    "    \n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"GarageYrBltBin\", None, None)\n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"YearBuiltBin\", None, None)\n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"YearRemodAddBin\", None, None)\n",
    "\n",
    "    return onehot_df\n",
    "\n",
    "# Add the one-hot encoded categorical features.\n",
    "onehot_df = munge_onehot(train_df)\n",
    "onehot_df = onehot(onehot_df, neighborhood_bin_train, \"NeighborhoodBin\", None, None)\n",
    "train_df_munged = train_df_munged.join(onehot_df)\n",
    "\n",
    "# These onehot columns are missing in the test data, so drop them from the\n",
    "# training data or we might overfit on them.\n",
    "drop_cols = [\n",
    "                \"_Exterior1st_ImStucc\", \"_Exterior1st_Stone\",\n",
    "                \"_Exterior2nd_Other\",\"_HouseStyle_2.5Fin\", \n",
    "            \n",
    "                \"_RoofMatl_Membran\", \"_RoofMatl_Metal\", \"_RoofMatl_Roll\",\n",
    "                \"_Condition2_RRAe\", \"_Condition2_RRAn\", \"_Condition2_RRNn\",\n",
    "                \"_Heating_Floor\", \"_Heating_OthW\",\n",
    "\n",
    "                \"_Electrical_Mix\", \n",
    "                \"_MiscFeature_TenC\",\n",
    "                \"_GarageQual_Ex\", \"_PoolQC_Fa\"\n",
    "            ]\n",
    "train_df_munged.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "onehot_df = munge_onehot(test_df)\n",
    "onehot_df = onehot(onehot_df, neighborhood_bin_test, \"NeighborhoodBin\", None, None)\n",
    "test_df_munged = test_df_munged.join(onehot_df)\n",
    "\n",
    "# This column is missing in the training data. There is only one example with\n",
    "# this value in the test set. So just drop it.\n",
    "test_df_munged.drop([\"_MSSubClass_150\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop these columns. They are either not very helpful or they cause overfitting.\n",
    "drop_cols = [\n",
    "    \"_Condition2_PosN\",    # only two are not zero\n",
    "    \"_MSZoning_C (all)\",\n",
    "    \"_MSSubClass_160\",\n",
    "]\n",
    "train_df_munged.drop(drop_cols, axis=1, inplace=True)\n",
    "test_df_munged.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# We take the log here because the error metric is between the log of the\n",
    "# SalePrice and the log of the predicted price. That does mean we need to \n",
    "# exp() the prediction to get an actual sale price.\n",
    "label_df = pd.DataFrame(index = train_df_munged.index, columns=[\"SalePrice\"])\n",
    "label_df[\"SalePrice\"] = np.log(train_df[\"SalePrice\"])\n",
    "\n",
    "print(\"Training set size:\", train_df_munged.shape)\n",
    "print(\"Test set size:\", test_df_munged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('XGBoost score on training set: ', 0.068078046016012944)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost -- I did some \"manual\" cross-validation here but should really find\n",
    "# these hyperparameters using CV. ;-)\n",
    "\n",
    "xgb_regr = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42,\n",
    "                 silent=1)\n",
    "\n",
    "xgb_regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_train_pred_xgb = xgb_regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"XGBoost score on training set: \", rmse(y_test, y_train_pred_xgb))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_test_pred_xgb = xgb_regr.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Xgboost gridsearch\n",
    "\n",
    "# xtreme_forest = xgb.XGBRegressor()\n",
    "\n",
    "# parameter_grid = {'max_depth' : [4,5,6],\n",
    "#                  'n_estimators':[100]\n",
    "#                  #'n_estimators': [200,210,240,250],\n",
    "#                  #'min_child_weight': [1,2,3,4]\n",
    "#                  }\n",
    "\n",
    "# cross_validation = StratifiedKFold(np.array(label_df['SalePrice']), n_folds=5)\n",
    "\n",
    "# grid_search_xgboost = GridSearchCV(xtreme_forest,\n",
    "#                            param_grid = parameter_grid,\n",
    "#                            scoring = 'mean_squared_error',\n",
    "#                            cv = cross_validation)\n",
    "\n",
    "# grid_search_xgboost.fit(train_df_munged, label_df)\n",
    "\n",
    "# print('Best score (Xgboost): {}'.format(grid_search_xgboost.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search_xgboost.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lasso score on training set: ', 0.10250920793562379)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# I found this best alpha through cross-validation.\n",
    "best_alpha = 0.00099\n",
    "\n",
    "lasso_regr = Lasso(alpha=best_alpha, max_iter=50000)\n",
    "lasso_regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_train_pred_lasso = lasso_regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"Lasso score on training set: \", rmse(y_test, y_train_pred_lasso))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_test_pred_lasso = lasso_regr.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Random Forest\n",
    "\n",
    "# rf_regr = RandomForestRegressor(n_estimators=5000, max_depth=8, max_features='sqrt',min_samples_split=20,n_jobs=-1)\n",
    "# rf_regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# # Run prediction on training set to get a rough idea of how well it does.\n",
    "# y_train_pred_rf = rf_regr.predict(train_df_munged)\n",
    "# y_test = label_df\n",
    "# print(\"Random Forest score on training set: \", rmse(y_test, y_train_pred_rf))\n",
    "\n",
    "# # Run prediction on the Kaggle test set.\n",
    "# y_test_pred_rf = rf_regr.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Extra Trees Regressor\n",
    "\n",
    "# et_regr = ExtraTreesRegressor(n_estimators=5000, max_depth=8, max_features='sqrt',n_jobs=-1)\n",
    "# et_regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# # Run prediction on training set to get a rough idea of how well it does.\n",
    "# y_pred = et_regr.predict(train_df_munged)\n",
    "# y_test = label_df\n",
    "# print(\"Extra Trees Regressor score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# # Run prediction on the Kaggle test set.\n",
    "# y_pred_et = et_regr.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wannjiun/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score (Kernel Ridge): -0.014089026792\n",
      "Best parameters: {'alpha': 0.1, 'degree': 2, 'kernel': 'polynomial'}\n"
     ]
    }
   ],
   "source": [
    "# Kernel Ridge GridSearch\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "kridge_grid = KernelRidge()\n",
    "\n",
    "parameter_grid = {'alpha': [0.0001,0.001,0.01,0.1],\n",
    "                 'degree': [1,2,3,4],\n",
    "                 'kernel': ['polynomial']\n",
    "                 #'n_estimators': [200,210,240,250],\n",
    "                 #'min_child_weight': [1,2,3,4]\n",
    "                 }\n",
    "\n",
    "cross_validation = StratifiedKFold(np.array(label_df['SalePrice']), n_folds=10)\n",
    "\n",
    "grid_search_kridge = GridSearchCV(kridge_grid,\n",
    "                           param_grid = parameter_grid,\n",
    "                           scoring = 'mean_squared_error',\n",
    "                           cv = cross_validation)\n",
    "\n",
    "grid_search_kridge.fit(train_df_munged, label_df)\n",
    "\n",
    "print('Best score (Kernel Ridge): {}'.format(grid_search_kridge.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search_kridge.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kernel Ridge score on training set: ', 0.065107326284677788)\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_kridge = grid_search_kridge.best_estimator_.predict(train_df_munged)\n",
    "print(\"Kernel Ridge score on training set: \", rmse(label_df,y_train_pred_kridge))\n",
    "y_test_pred_kridge = grid_search_kridge.best_estimator_.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Elastic Net Grid Search\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# elastic_grid = ElasticNet()\n",
    "\n",
    "# parameter_grid = {'alpha': [0.001,0.01,0.1,0.5,1],\n",
    "#                  'l1_ratio': [0.01,0.1,0.5,1],\n",
    "#                  'fit_intercept': [True, False],\n",
    "#                  'normalize': [True,False]}\n",
    "\n",
    "# cross_validation = StratifiedKFold(np.array(label_df['SalePrice']), n_folds=10)\n",
    "\n",
    "# grid_search_elastic = GridSearchCV(elastic_grid,\n",
    "#                            param_grid = parameter_grid,\n",
    "#                            scoring = \"mean_squared_error\",\n",
    "#                            cv = cross_validation)\n",
    "\n",
    "# grid_search_elastic.fit(train_df_munged, label_df)\n",
    "\n",
    "# print('Best score (ElasticNet): {}'.format(grid_search_elastic.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search_elastic.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_train_pred_elastic = grid_search_elastic.best_estimator_.predict(train_df_munged)\n",
    "# print(\"Elastic Net score on training set: \", rmse(label_df,y_train_pred_elastic))\n",
    "# y_test_pred_elastic = grid_search_elastic.best_estimator_.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gbm_grid = GradientBoostingRegressor(max_features='sqrt')\n",
    "\n",
    "# parameter_grid = {'max_depth' : [4,6,8],\n",
    "#                  'n_estimators': [5000],\n",
    "#                  'min_samples_split':[5,10,15,20]}\n",
    "\n",
    "# cross_validation = StratifiedKFold(label_df['SalePrice'], n_folds=5)\n",
    "\n",
    "# grid_search_gbm = GridSearchCV(gbm_grid,\n",
    "#                            param_grid = parameter_grid,\n",
    "#                            cv = cross_validation)\n",
    "\n",
    "# grid_search_gbm.fit(train_df_munged, label_df)\n",
    "\n",
    "# print('Best score (GradientBoostingRegressor): {}'.format(grid_search_gbm.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search_gbm.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_train_pred_gbm = grid_search_gbm.best_estimator_.predict(train_df_munged)\n",
    "# print(\"GBM score on training set: \", rmse(label_df,y_train_pred_gbm))\n",
    "# y_test_pred_gbm = grid_search_gbm.best_estimator_.predict(test_df_munged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Analyzing the Weighting performances\n",
    "# ## XGboost + Lasso + Kernel Ridge\n",
    "# rmse_arr = np.empty([5,5,5,5])\n",
    "\n",
    "# for wt_xgb in np.arange(0,5):\n",
    "#     for wt_lasso in np.arange(0,5):\n",
    "#         for wt_kridge in np.arange(0,5):\n",
    "#             for wt_elastic in np.arange(0,5):\n",
    "#                 if(wt_lasso + wt_kridge + wt_xgb == 0):\n",
    "#                     rmse_arr[wt_xgb,wt_lasso,wt_kridge,wt_elastic] = 0.1\n",
    "#                     continue\n",
    "#                 #for wt_gbm in np.arange(0,4):\n",
    "#                 y_pred_avg = (wt_xgb*np.ravel(y_train_pred_xgb) + wt_lasso*np.ravel(y_train_pred_lasso) + wt_kridge*np.ravel(y_train_pred_kridge) + wt_elastic*np.ravel(y_train_pred_elastic))\n",
    "#                 y_pred_avg = y_pred_avg / (wt_xgb + wt_lasso + wt_kridge+wt_elastic)\n",
    "#                 if rmse(label_df,y_pred_avg) < 0.0005:\n",
    "#                     rmse_arr[wt_xgb,wt_lasso,wt_kridge,wt_elastic] = 0.1\n",
    "#                 else:    \n",
    "#                     rmse_arr[wt_xgb,wt_lasso,wt_kridge,wt_elastic] = rmse(label_df,y_pred_avg)\n",
    "#                 print(\"RMSE Training Error for ::wt_xgb = %s, wt_lasso = %s, wt_kridge = %s, wt_elastic = %s:: = %s\\n\" %(str(wt_xgb), str(wt_lasso), str(wt_kridge), str(wt_elastic),str(1000*rmse(label_df,y_pred_avg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Lasso + Kernel Ridge Weight Analysis\n",
    "# rmse_arr = np.zeros([5,5])\n",
    "\n",
    "# for wt_lasso in np.arange(0,5):\n",
    "#     for wt_kridge in np.arange(0,5):\n",
    "#         if(wt_lasso + wt_kridge == 0):\n",
    "#             rmse_arr[wt_lasso,wt_kridge] = 0.1\n",
    "#             continue\n",
    "#         #for wt_gbm in np.arange(0,4):\n",
    "#         y_pred_avg = (wt_lasso*np.ravel(y_train_pred_lasso) + wt_kridge*np.ravel(y_train_pred_kridge))\n",
    "#         y_pred_avg = y_pred_avg / (wt_lasso + wt_kridge)\n",
    "#         rmse_arr[wt_lasso,wt_kridge] = rmse(label_df,y_pred_avg)\n",
    "#         print(\"RMSE Training Error for ::wt_lasso=%s, wt_kridge=%s:: = %s\\n\" %(str(wt_lasso), str(wt_kridge),  str(1000*rmse(label_df,y_pred_avg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Analyzing the error\n",
    "# ravel_rmse = 1000*np.ravel(rmse_arr)\n",
    "# ravel_rmse = ravel_rmse[ravel_rmse>1]\n",
    "# #sorted_rmse = sorted_rmse[sorted_rmse<100000]\n",
    "# print(np.max(ravel_rmse))\n",
    "# print(np.min(ravel_rmse))\n",
    "# plt.plot(ravel_rmse)#, np.linspace(0,1,sorted_rmse.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(rmse_arr.shape)\n",
    "\n",
    "# print(\"Min = \", np.min(rmse_arr))\n",
    "# print(np.where(rmse_arr == np.min(rmse_arr)))\n",
    "\n",
    "# print(\"Max = \", np.max(rmse_arr))\n",
    "# print(np.where(rmse_arr == np.max(rmse_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final = (1*np.ravel(y_test_pred_xgb) + 1*np.ravel(y_test_pred_kridge) + 1*np.ravel(y_test_pred_lasso))/3\n",
    "y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "#     \"\"\"Logloss, i.e. the score of the bioresponse competition.\n",
    "#     \"\"\"\n",
    "#     attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "#     return - np.mean(actual * np.log(attempt) +\n",
    "#                      (1.0 - actual) * np.log(1.0 - attempt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "# n_folds = 10\n",
    "# verbose = True\n",
    "# shuffle = False\n",
    "\n",
    "# X_ens = np.array(train_df_munged)\n",
    "# y_ens = np.array(label_df['SalePrice'])\n",
    "# X_submission = np.array(test_df_munged)\n",
    "\n",
    "# if shuffle:\n",
    "#     idx = np.random.permutation(y.size)\n",
    "#     X_ens = train_df_munged[idx]\n",
    "#     y_ens = targets_munged[idx]\n",
    "\n",
    "# skf = list(StratifiedKFold(y_ens, n_folds))\n",
    "\n",
    "# clfs = [lasso_regr,\n",
    "#         xgb_regr,\n",
    "#         #RandomForestRegressor(n_estimators=5000, max_depth=4, max_features='sqrt',n_jobs=-1),\n",
    "#         #ExtraTreesRegressor(n_estimators=5000, max_depth=4,n_jobs=-1),\n",
    "#         GradientBoostingRegressor(learning_rate=0.05, subsample=0.5, max_features='sqrt',max_depth=4, n_estimators=5000),\n",
    "#         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Creating train and test sets for blending\n",
    "\n",
    "# dataset_blend_train = np.zeros((X_ens.shape[0], len(clfs)))\n",
    "# dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for j, clf in enumerate(clfs):\n",
    "#     print(j, clf)\n",
    "#     dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "#     for i, (train_curr, test_curr) in enumerate(skf):\n",
    "#         print(\"Fold\", i)\n",
    "#         X_train = X_ens[train_curr]\n",
    "#         y_train = y_ens[train_curr]\n",
    "#         X_test = X_ens[test_curr]\n",
    "#         y_test = y_ens[test_curr]\n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_submission = clf.predict(X_test)\n",
    "#         dataset_blend_train[test_curr, j] = y_submission\n",
    "#         dataset_blend_test_j[:, i] = clf.predict(X_submission)\n",
    "#         dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "# print (\"Blending over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #y_final = np.mean(dataset_blend_test,axis=1)\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# dd = KernelRidge()\n",
    "# dd.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "# linear_grid = LinearRegression()\n",
    "\n",
    "# parameter_grid = {\n",
    "#                  'fit_intercept': [True,False],\n",
    "#                  'normalize':[True,False]\n",
    "#                  #'n_estimators': [200,210,240,250],\n",
    "#                  #'min_child_weight': [1,2,3,4]\n",
    "#                  }\n",
    "\n",
    "# cross_validation = StratifiedKFold(np.array(label_df['SalePrice']), n_folds=5)\n",
    "\n",
    "# grid_search_linear = GridSearchCV(linear_grid,\n",
    "#                            param_grid=parameter_grid,\n",
    "#                            scoring= \"r2\",\n",
    "#                            cv=cross_validation)\n",
    "\n",
    "# grid_search_linear.fit(dataset_blend_train, y_ens)\n",
    "\n",
    "# print('Best score: {}'.format(grid_search_linear.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search_linear.best_params_))\n",
    "\n",
    "# #clf.fit(dataset_blend_train, y_ens)\n",
    "# #y_final = clf.predict(dataset_blend_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# kridge_grid = KernelRidge()\n",
    "\n",
    "# parameter_grid = {\n",
    "#                  'alpha': [0.0001,0.001,0.01,0.1],\n",
    "#                  'degree': [1,2,3,4],\n",
    "#                  'kernel': ['polynomial']\n",
    "#                  #'n_estimators': [200,210,240,250],\n",
    "#                  #'min_child_weight': [1,2,3,4]\n",
    "#                  }\n",
    "\n",
    "# cross_validation = StratifiedKFold(np.array(label_df['SalePrice']), n_folds=10)\n",
    "\n",
    "# grid_search_kridge = GridSearchCV(kridge_grid,\n",
    "#                            param_grid=parameter_grid,\n",
    "#                            scoring= \"neg_mean_squared_error\",\n",
    "#                            cv=cross_validation)\n",
    "\n",
    "# grid_search_kridge.fit(dataset_blend_train, y_ens)\n",
    "\n",
    "# print('Best score: {}'.format(grid_search_kridge.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search_kridge.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search_kridge.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# ensemble_clfs_0106 = clfs\n",
    "\n",
    "# with open('0106_objs.pckl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([train_df_munged,test_df_munged,ensemble_clfs_0106,dataset_blend_test,dataset_blend_train], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_final = grid_search_kridge.predict(dataset_blend_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.exp(y_final)\n",
    "\n",
    "# Final Conversion.\n",
    "output_file = 'xgboost_lasso_kridge_weights_1_1_1'\n",
    "final_file = '0108_'+ output_file +'.csv'\n",
    "\n",
    "pred_df = pd.DataFrame(y_pred, index=test_df[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.shape\n",
    "pred_df.to_csv(path+final_file, header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
